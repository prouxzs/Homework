---
title: "Homework"
author: "Zach Proux"
date: "2/2/2018"
output: html_document
---

Assignment 1 - basics

---
```{r}
library(knitr)
opts_knit$set(root.dir='../')
```

```{r setup, include=FALSE}
tgpp = read.csv("./Data/tgpp.csv")
```
Q1) What are the names of the columns in this dataset?
```{r, eval=FALSE}
tgpp
```
A1) plot, year, record_id, corner, scale, richness, eastin, northing, slope, ph, yrsslb

Q2) How many rows and columns does this data file have?
A2) 4,080

Q3) What kind of object is each data column? Hint: checkout the function sapply().
A3) 
```{r}
sapply(tgpp, class)
```
Q4) What are the values of the the datafile for rows 1, 5, and 8 at columns 3, 7, and 10?
A4) 
```{r}
tgpp[c(1,5,8),c(3,7,10)]
```
Q5) Create a pdf of the relationship between the variables "scale" and "richness". Scale is the area in square meters of the quadrat in which richness was recorded. Be sure to label your axes clearly, and choose a color you find pleasing for the points. To get a list of available stock colors use the function colors(). 
A5) 
```{r}
pdf("tgpp_plot.pdf")
plot(tgpp$richness~tgpp$scale,xlab="Scale",ylab="Richness",col="darkgreen")
dev.off()
```
```{r}
plot(tgpp$richness~tgpp$scale,xlab="Scale",ylab="Richness",col="darkgreen")
```

Q6) What happens to your plot when you set the plot argument log equal to 'xy'. plot(..., log='xy')
A6) 
```{r}
plot(tgpp$richness~tgpp$scale,xlab="Scale",ylab="Richness",col="darkgreen", log='xy')
```



Assignment 2 - for loops



1) Describe the values stored in the object output. In other words what did the loops create?

   - The loop created a matrix with the average values of each column for each species.


2) Describe using pseudo-code how `output` was calculated.
```{r, eval = FALSE}
sp_ids = unique(iris$Species)
# Create an object of unrepeated species names in the iris data frame.
output = matrix(0, nrow=length(sp_ids), ncol=ncol(iris)-1)
# Create an object, specifically a matrix called 'output', with the number of rows defined by 
# the number of unique species ids and the number of columns defined by the number of columns
# in iris except for the last one which was "Species."  The 0 indicates that the matrix is 
# empty at this point.
rownames(output) = sp_ids
colnames(output) = names(iris[ , -ncol(iris)])
# Populates 'output' with row names from 'sp_ids' and column names from 'iris' except for the 
# 5th column - species.

for(i in seq_along(sp_ids)) {
# Generate a regular sequence of species ids and assign i as the variable to refer to that sequence.
# "Do the following for each species one at a time."
    iris_sp = subset(iris, subset=Species == sp_ids[i], select=-Species)
# Create a data frame for each unique species id that excludes the Species column.
    for(j in 1:(ncol(iris_sp))) {
# Create a vector of 1:4 (number of columns in object 'iris_sp') and use j to refer to it.
        x = 0
        y = 0
# Create two objects, x and y, both defined as equal to 0.
        if (nrow(iris_sp) > 0) {
# Perform the function that follows if there are more than 0 rows in 'iris_sp'
            for(k in 1:nrow(iris_sp)) {
# Create a vector of 1:50 (number of rows in 'iris_sp') and use k to refer to it.
                x = x + iris_sp[k, j]
# Define x as the sum of all numbers in 'iris_sp'. 
                y = y + 1
# Define y such that it is a running count of how many values were added together.
            }
            output[i, j] = x / y 
# Populate 'output' with the averages for each of the 3 species in each of the 4 columns
        }
    }
}
output
```


3) The variables in the loop were named so as to be vague. How can the objects output, x, and y could be renamed such that it is clearer what is occurring in the loop?

   - output could be renamed 'averages' so the user knows what was calculated. x could be renamed 'sum' to specify it's adding values together and y could be renamed 'count' to indicate it is keeping count of how many values were added.
   
   
4) It is possible to accomplish the same task using fewer lines of code? Please suggest one other way to calculate output that decreases the number of loops by 1.
```{r}
data(iris)

sp_ids = unique(iris$Species)

output = matrix(0, nrow=length(sp_ids), ncol=ncol(iris)-1)
rownames(output) = sp_ids
colnames(output) = names(iris[ , -ncol(iris)])

for(i in seq_along(sp_ids)) {
    iris_sp = subset(iris, subset=Species == sp_ids[i], select=-Species)
    for(j in 1:(ncol(iris_sp))) 
            output[i, j] = sum(iris_sp[, j]) / nrow(iris_sp) 
        }
output
```


5) You have a vector x with the numbers 1:10. Write a for loop that will produce a vector y that contains the sum of x up to that index of x. So for example the elements of x are 1, 2, 3, and so on and the elements of y would be 1, 3, 6, and so on.

```{r}
x = 1:10
y = vector("integer",10)
for(l in 1:length(y)) {
  y[l] = sum(x[1:l])
}
y
```


6) Modify your for loop so that if the sum is greater than 10 the value of y is set to NA

```{r}
x = 1:10
y = vector("integer",10)
for(l in 1:length(y)) {
  y[l] = sum(x[1:l])
      if(y[l] > 10) {
        y[l] = NA
      }
}
y
```


7) Place your for loop into a function that accepts as its argument any vector of arbitrary length and it will return y.

```{r}
sum_vec = function(x) {
  for(l in 1:length(x)) {
  y[l] = sum(x[1:l])
  }
     return(y)
}
# Sum_vec functionality test
test = 1:20
sum_vec(test)
```




Assignment 3 - Univariate



Univariate Assignment

  Read in tree data, metadata can be found in: `./data/tree_metadata.txt`

```{r}
library(knitr)
opts_knit$set(root.dir='../')
trees = read.csv('../Data/treedata.csv')
```

1 Carry out an exploratory analysis using the tree dataset. 
  Develop and compare models for species cover for a habitat generalist
  [_*Acer rubrum*_ (Red maple)](http://www.durhamtownship.com/blog-archives/pix/November1407.jpg) 
  and a habitat specialist [_*Abies fraseri*_ (Frasier   fir)]    
  (https://upload.wikimedia.org/wikipedia/commons/d/d0/Abies_fraseri_Mitchell.jpg). 
   
  Because this dataset includes both continuous and discrete explanatory variables 
  use the function `Anova` in the packages `car` 

```{r}
library(car)
```

```{r}
tree_rm = trees[trees$spcode=="ACERRUB",]
tree_ff = trees[trees$spcode=="ABIEFRA",]
```

```{r, eval = FALSE}
#generalist model
rm_mod = lm(tree_rm$cover ~ tree_rm$plotsize + tree_rm$utme + tree_rm$utmn + 
            tree_rm$elev + tree_rm$tci + tree_rm$streamdist + tree_rm$disturb +
            tree_rm$beers)
summary(rm_mod)
#significant explanatory variables: plotsize, utmn, elev, streamdist, and beers.
```

```{r, eval = FALSE}
rm_mod = update(rm_mod, ~ . - utme - tci - disturb, data = tree_rm)
summary(rm_mod)
Anova(rm_mod, type = 3)
```


```{r, eval = FALSE}
#specialist model
ff_mod = lm(tree_ff$cover ~ tree_ff$plotsize + tree_ff$utme + tree_ff$utmn + 
            tree_ff$elev + tree_ff$tci + tree_ff$streamdist + tree_ff$disturb + 
            tree_ff$beers)
summary(ff_mod)
#significant explanatory variables: utmn, elevation, tci, and beers.
```

```{r, eval = FALSE}
ff_mod = update(ff_mod, ~ . - tree_ff$streamdist - tree_ff$disturb - tree_ff$utme
                - tree_ff$plotsize)
summary(ff_mod)
Anova(ff_mod, type = 3)
```

  Compare the p-values you observe using the function `Anova` to those generated
  using `summary`. 

          The p-values are the same in summary and in the 'Anova' table.

  For each species address the following additional questions:

  How well does the exploratory model appear to explain cover?
        
          Our generalist model explains 4.1% of variation in cover and our
          specialist model explains 41.2% of variation in cover.
        
  Which explanatory variables are the most important?

          Elevation is the most important in both models.  In the generalist 
          model, beers and streamdist were also important.  In the specialist
          model, tci and beers were important.

  Do model diagnostics indicate any problems with violations of OLS assumptions?
        
```{r, eval = FALSE}
#From the residual plots below, it doesn't look like any of the assumptions are violated.
plot(rm_mod)
plot(ff_mod)
```

  Are you able to explain variance in one species better than another, why might
  this be the case?
  
          Yes, we can explain variance in the specialist much better which makes
          perfect sense because specialists tend to be tolerant of a much narrower
          range of conditions.  This also means their characteristics might be 
          heavily dependent on just a few variables as opposed to generalists
          that can live in a variety of habitats and be effected by a variety of
          different variables.

2 You may have noticed that the variable cover is defined as positive integers 
  between 1 and 10. and is therefore better treated as a discrete rather than 
  continuous variable. 
   
  Re-examine your solutions to the question above but from the perspective of a 
  General Linear Model (GLM) with a Poisson error term (rather than a Gaussian
  one as in OLS). 
  
  The Poisson distribution generates integers 0 to positive infinity so this may 
  provide a good first approximation. 
  
  Your new model calls will look as follows:
```{r}
rm_glm = glm(cover ~ plotsize + utmn + elev + tci + streamdist + beers, 
             data = tree_rm, family='poisson')
summary (rm_glm)
```

```{r}
ff_glm = glm(cover ~ tci + utmn + elev + beers, data = tree_ff, family='poisson')
summary (ff_glm)
```

  For assessing the degree of variation explained you can use a pseudo-R-squared
  statistic (note this is just one of many possible)

```{r}
    pseudo_r2 = function(glm_mod) {
        1 -  glm_mod$deviance / glm_mod$null.deviance
    }
pseudo_r2(rm_glm)
pseudo_r2(ff_glm)
```

  Compare the residual sums of squares between the traditional OLS and glm models
  using `anova` (Note: not `Anova`) 
  
         The change in residual sum of swaures was negligible, although I can't
         tell if my anova() ran correctly because it returned this statement:
         "Models with response cover removed because response differs from model 1"

```{r, eval = FALSE}
anova(rm_mod, rm_glm)
anova(ff_mod, ff_glm)
```

  Does it appear that changing the error distribution changed the results much? 
  In what ways? 

          The pseudo_r2 values in the GLM were slightly higher than the adjusted 
          r2 values from the OLS model, but only by 4% in the specialist model 
          and 0.4% in the generalist model.

3 Provide a plain English summary (i.e., no statistics) of what you have found
  and what conclusions we can take away from your analysis?
  
         We learned that can explain about 45% of the variation in cover of a 
         specialist tree (Abies fraseri), but only about 4% in a generalist
         (Acer rubrum).  We learned the residuals more closely follow a poisson
         distribution than a gaussian one, but only slightly.  We also learned 
         elevation is the most important variable in explaining variation in cover
         in both species, but only because we don't have the variables that 
         explain the majority of the variation in our models.

4 (optional) Examine the behavior of the function `step()` using the exploratory
  models developed above. This is a very simple and not very robust machine 
  learning stepwise algorithm that uses AIC to select a best model. By default
  it does a backward selection routine. 

5 (optional) Develop a model for the number of species in each site 
  (i.e., unique plotID). This variable will also be discrete so the Poisson
  may be a good starting approximation. Side note: the Poisson distribution 
  converges asymptotically on the Gaussian distribution as the mean of the 
  distribution increases. Thus Poisson regression does not differ much from 
  traditional OLS when means are large. 
  
  
#*Ran into a problem when I was knitting my pdf.  All of the chunks ran correctly*
#*but for some reason it kept getting stuck at any lines with anova functions so*
#*I coded 'eval = FALSE' and that got the knit to work*


Assignment 4 - Multivariate


For this assignment will be analyzing data on the Vegetation
and Environment in Dutch Dune Meadows.

To import the data and read the metadata run the following:

```{r}
library(vegan)
data(dune)
data(dune.env)
?dune
```

1. Conduct an indirect ordination on the dune plant community. Specifically,
visually examine a NMDS plot using the bray-curtis distance metric. Below is 
some code to help you develop a potential plot that emphasizes the role of the
environmental variable "Moisture". 

```{r}
mds_dune = metaMDS(dune, k=2, trymax=100)
plot(mds_dune, type='n')
text(mds_dune, 'sp', cex=.5)
# generate vector of colors 
color_vect = rev(terrain.colors(6))[-1]
points(mds_dune, 'sites', pch=19, 
        col=color_vect[dune.env$Moisture])
legend('topright', paste("Moisture = ", 1:5, sep=''), 
        col=color_vect, pch=19)
```

Describe how you interpret the graphic. 

    The x-axis is the dominant axis of variation.  To interpret the plot, we need
    to have some knowledge of the data.  For example, because we know moisture 
    affects community assemblage we can look at the plot and see that some species
    were associated with very high moisture and also have higher x values and other 
    species were associated with dry areas and also have lower x values.  

What is the goal of creating such a plot? 

    This approach is used when we are not quite sure what explanatory variables
    are important in explaining observed variation and we just want to visualize
    differences in a matrix of explanatory and dependent variables.  Once we visualize
    it, the idea is your background knowledge might allow you to identify a trend
    in the multidimensional distribution.

Does this analysis suggest any interesting findings with respect to the dune vegetation?

    This analysis suggests moisture may be the most important variable in explaining
    the variation in presence of meadow vegation.


2. Carry out a direct ordination using CCA in order to test any potential 
hypotheses that you developed after examining the MDS plot. Specifically,
carry out a test of the entire model (i.e., including all constrained axes)
and also carry out tests at the scale of individual explanatory variables
you included in your model if you included more than one variable. Plot your 
results.

```{r}
cca_dune = cca(dune ~ ., data = dune)
cca_dune
RsquareAdj(cca_dune, 1000)
anova(cca_dune, permutations = 1000)
anova(cca_dune, by ='margin', permutations = 1000)
plot(cca_dune, type = 'n', scaling = 1)
orditorp(cca_dune, display = 'sp', cex = 0.5, scaling = 1, col = 'blue')
text(cca_dune, display = 'bp', col = 'red')
```

```{r}
rda_dune = rda(dune ~ . , data = dune)
rda_dune
RsquareAdj(rda_dune)
plot(rda_dune, type = 'n', scaling = 1)
orditorp(rda_dune, display = 'sp', cex = 0.5, scaling = 1, col = 'blue')
text(rda_dune, display = 'cn', col = 'red')
```


3. Do your two analyses agree with one another or complement one another or do
these two analyses seem to be suggesting different take home messages? Which
analysis do you find to be more useful?

    I'm not confident in my understanding of this, but I think the analyses agree
    with one another.  They both plot the species in multidimensional space in 
    similar groupings: agrostol and alopgeni opposite anthodor and planlanc.  The
    nmds was more helpful however, because it actually showed us that moisture 
    could be responsible for the viariation as opposed to the cca and rda which 
    just group the species based on the characteristics of the plots they occurred
    in.  
